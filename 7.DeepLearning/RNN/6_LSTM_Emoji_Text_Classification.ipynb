{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVmZOWRrBF8g"
   },
   "source": [
    "# Problem: RNN Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkJKxn3wgkvM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, LSTM, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJPA0QLqB1XP"
   },
   "source": [
    "### Dataset EMOJISET\n",
    "\n",
    "Tiny dataset (X, Y) where:\n",
    "- X contains 132 sentences (strings)\n",
    "- Y contains a integer label between 0 and 4 corresponding to an emoji for each sentence\n",
    "\n",
    "<img src=\"https://github.com/Alireza-Akhavan/rnn-notebooks/blob/master/images/data_set.png?raw=1\" style=\"width:700px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxUPiZirgv3A"
   },
   "outputs": [],
   "source": [
    "# Read csv file\n",
    "def read_csv(file_name):\n",
    "  data_frame = pd.read_csv(file_name)\n",
    "  X = np.array(data_frame[\"sentence\"])\n",
    "  Y = np.array(data_frame[\"label\"], dtype=int) # labels are integere\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odVepcWahpOr"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv(\"datasets/Emoji_Text_Classification/train.csv\")\n",
    "X_test, Y_test = read_csv(\"datasets/Emoji_Text_Classification/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBTDPX1Xhs8U",
    "outputId": "2d6de59b-5152-48ff-b2bb-4986791fceaa"
   },
   "outputs": [],
   "source": [
    "# Get max length of sentences\n",
    "max_len = len(max(X_train, key=len).split(\" \"))\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Z3RPSgFhrDv",
    "outputId": "77723ec4-4a9f-40c5-df3b-05ee08b4b181"
   },
   "outputs": [],
   "source": [
    "# Replace labels with related emoji\n",
    "def label_to_emoji(label):\n",
    "    emojies = [\"‚ù§Ô∏è\", \"üèê\", \"üòÑ\", \"üòû\", \"üç¥\"]\n",
    "    return emojies[label]\n",
    "\n",
    "index = 5\n",
    "print(X_train[index], label_to_emoji(Y_train[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dfohLRuKXl-",
    "outputId": "7d74c461-77ca-4f33-bd13-2944f5bed3cf"
   },
   "outputs": [],
   "source": [
    "# Number of sentence in each class\n",
    "unique, counts = np.unique(Y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LflrvFq-FXdJ"
   },
   "source": [
    "## Emojifier-V1\n",
    "\n",
    "Each word has some feature, and in Emojifier-V1 we want to classify sentences using multilayer perceptron:\n",
    "\n",
    "- We get the average of words in each sentence and then forward it to the multilayer perceptron with 50 input neurons(each word has 50 features, then the average of words in the sentence has 50 features) and an output layer of softmax with 5 neurons.\n",
    "\n",
    "- For feature vectors, we can get from this link: http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://github.com/Alireza-Akhavan/rnn-notebooks/blob/master/images/image_1.png?raw=1\" style=\"width:900px;height:300px;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USU58amfjtaQ"
   },
   "outputs": [],
   "source": [
    "# Convert labels to one hot\n",
    "num_classes = len(np.unique(Y_train))\n",
    "\n",
    "Y_train_oh = tf.keras.utils.to_categorical(Y_train, num_classes)\n",
    "Y_test_oh = tf.keras.utils.to_categorical(Y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKJfkEXBPq6J",
    "outputId": "b21b0cce-988d-446d-8f63-1bf877146464"
   },
   "outputs": [],
   "source": [
    "index = 5\n",
    "print(Y_train[index], \"is converted into one hot\", Y_train_oh[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2YXsxC5hvvF"
   },
   "outputs": [],
   "source": [
    "# Download and extract glove.6B for feature vectors \n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip -d glov.6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtQ_L_0Tiqcm"
   },
   "outputs": [],
   "source": [
    "# Read feature vectors and save them\n",
    "\"\"\"\n",
    "In the text file, in each line,\n",
    "the word comes first, and then the feature vectors(each word is in one line).\n",
    "\"\"\"\n",
    "def read_glov_vectors(glove_file):\n",
    "  f = open(glove_file, encoding=\"utf8\")\n",
    "  words_to_vec = dict()\n",
    "  for line in f:\n",
    "    line = line.strip().split()\n",
    "    word = line[0]\n",
    "    vec = line[1:]\n",
    "    words_to_vec[word] = np.array(vec, dtype=np.float64)\n",
    "  return words_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JG-Yx2ixlshd"
   },
   "outputs": [],
   "source": [
    "words_to_vec = read_glov_vectors(\"glove.6B/glove.6B.50d.txt\")\n",
    "\n",
    "# Test the output of read_glov_vectors function\n",
    "words_to_vec[\"hello\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPCRo5VumAQH"
   },
   "outputs": [],
   "source": [
    "#  Convert sentences to the average of the word vectors\n",
    "def sentence_to_avg(sentence):\n",
    "  words = sentence.lower().split() # Convert uppercase to lowercase\n",
    "  sum_vectors = np.zeros((50, ))\n",
    "  for w in words:\n",
    "    sum_vectors += words_to_vec[w]\n",
    "  avg_vectors = sum_vectors / len(words)\n",
    "  return avg_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HnS59mns5x-"
   },
   "outputs": [],
   "source": [
    "# Test sentence_to_avg function\n",
    "sentence_to_avg(\"Pasta is my favorite food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyUggXaVnhpy",
    "outputId": "1a9bdb5b-4b1b-466f-aea1-1257524d6df5"
   },
   "outputs": [],
   "source": [
    "# Get the average of all sentences\n",
    "X_train_avg = []\n",
    "for i in range(X_train.shape[0]):\n",
    "  X_train_avg.append(sentence_to_avg(X_train[i]))\n",
    "\n",
    "X_train_avg = np.array(X_train_avg)\n",
    "\n",
    "X_train_avg.shape, Y_train_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tueM3X_HeWK"
   },
   "outputs": [],
   "source": [
    "# Create model(using perceptron)\n",
    "class EmojiNet_V1(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = Dense(num_classes, input_shape=(50,), activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjZk2ZJRHhKR"
   },
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "model = EmojiNet_V1()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_avg, Y_train_oh, epochs=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkIvwzRIHp1-",
    "outputId": "280ce784-f20b-45c3-e791-b227de9d2b33"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "X_test_avg = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test_avg.append(sentence_to_avg(X_test[i]))\n",
    "\n",
    "X_test_avg = np.array(X_test_avg)\n",
    "model.evaluate(X_test_avg, Y_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGsM6aAoHv2c",
    "outputId": "5e7a7c91-7268-41d0-f92a-1effff6661f3"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "X_me = np.array([\"not sad\", \"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy and funny\"])\n",
    "Y_me = np.array([[2], [0], [0], [2], [1], [4], [3]])\n",
    "X_me_avg = []\n",
    "\n",
    "for x in X_me:\n",
    "    X_me_avg.append(sentence_to_avg(x))\n",
    "\n",
    "X_me_avg = np.array(X_me_avg)\n",
    "pred = model.predict(X_me_avg)\n",
    "\n",
    "for i in range(X_me.shape[0]):\n",
    "    print(X_me[i], label_to_emoji(np.argmax(pred[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URuAB0T1llZs"
   },
   "source": [
    "## Emojifier-V2: Using RNNs: \n",
    "\n",
    "Let's build an LSTM model that takes as input word sequences. This model will be able to take word ordering into account. Emojifier-V2 will continue to use pre-trained word embeddings to represent words, but will feed them into an LSTM, whose job it is to predict the most appropriate emoji. \n",
    "\n",
    "Run the following cell to load the Keras packages.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://github.com/Alireza-Akhavan/rnn-notebooks/blob/master/images/emojifier-v2.png?raw=1\" style=\"width:700px;height:400px;\"> <br>\n",
    "<caption><center> **Figure 3**: Emojifier-V2. A 2-layer LSTM sequence classifier. </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cx-HnKjDTvNO"
   },
   "source": [
    "<img src=\"https://github.com/Alireza-Akhavan/rnn-notebooks/blob/master/images/embedding1.png?raw=1\" style=\"width:700px;height:250px;\">\n",
    "<caption><center> **Figure 4**: Embedding layer. This example shows the propagation of two examples through the embedding layer. Both have been zero-padded to a length of `max_len=5`. The final dimension of the representation is  `(2,max_len,50)` because the word embeddings we are using are 50 dimensional. </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iac4qtchIXRk"
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "class EmojiNet_V2(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gru_1 = GRU(128, return_sequences=True)\n",
    "        self.dropout_1 = Dropout(0.3)\n",
    "        self.gru_2 = GRU(256)\n",
    "        self.dropout_2 = Dropout(0.5)\n",
    "        self.dense = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.gru_1(x)\n",
    "        # x = self.dropout_1(x)\n",
    "        x = self.gru_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKURJ9USpxJR"
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model = EmojiNet_V2()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hU4iX2_WH6jc"
   },
   "outputs": [],
   "source": [
    "# Fix the size of all sentences to max_len\n",
    "def convert_sentences_to_embeddings(X):\n",
    "    emb_dim = words_to_vec[\"cucumber\"].shape[0]  # define dimensionality of your GloVe word vectors (= 50)\n",
    "    emb_matrix = np.zeros((X.shape[0], max_len, emb_dim))\n",
    "    for i in range(X.shape[0]):\n",
    "        words = X[i].lower().split()\n",
    "        for j in range(len(words)):\n",
    "            emb_matrix[i, j, :] = words_to_vec[words[j]]\n",
    "    return emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CwvU_7FH_XB"
   },
   "outputs": [],
   "source": [
    "# Test convert_sentences_to_embeddings function\n",
    "X_me = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "print(X_me)\n",
    "print(convert_sentences_to_embeddings(X_me))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ci8WjOaRITIN",
    "outputId": "78975bac-1f89-49c0-c7ae-d9d7da70758a"
   },
   "outputs": [],
   "source": [
    "# Run convert_sentences_to_embeddings function for training data \n",
    "X_train_embs =convert_sentences_to_embeddings(X_train)\n",
    "X_train_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZp85tH8IdGe"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_embs, Y_train_oh, epochs=100, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HPCVVYx8ycym",
    "outputId": "186c29c4-61d5-43a6-d201-5e299cd59199"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "X_test_embs = convert_sentences_to_embeddings(X_test)\n",
    "print(X_test_embs.shape)\n",
    "model.evaluate(X_test_embs, Y_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYvYfbwfysLn",
    "outputId": "dfa4ff63-7b63-4ce8-8dba-488b88323262"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "X_me = np.array([\"not happy\", \"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy and funny\"])\n",
    "Y_me = np.array([[2], [0], [0], [2], [1], [4], [3]])\n",
    "X_me_embed = convert_sentences_to_embeddings(X_me) \n",
    "\n",
    "pred = model.predict(X_me_embed)\n",
    "\n",
    "for i in range(X_me.shape[0]):\n",
    "    print(X_me[i], label_to_emoji(np.argmax(pred[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52IiygWLzRLq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

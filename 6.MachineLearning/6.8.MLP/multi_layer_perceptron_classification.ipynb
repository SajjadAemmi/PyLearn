{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1257, 64), (1257, 10), (540, 64), (540, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data\n",
    "Y = digits.target\n",
    "Y = np.eye(10)[Y]  # one hot\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "\n",
    "def cross_entropy_error(Y_pred, Y_gt):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(Y_gt * np.log(Y_pred + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = X_train.shape[1]\n",
    "H1 = 128\n",
    "H2 = 32\n",
    "D_out = Y_train.shape[1]\n",
    "\n",
    "η = 0.001\n",
    "epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, W2, W3 = np.random.randn(D_in, H1), np.random.randn(H1, H2), np.random.randn(H2, D_out)\n",
    "B1, B2, B3 = np.random.randn(H1), np.random.randn(H2), np.random.randn(D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss train: 5190.959281503772 acc train: 0.15194908512330946\n",
      "loss test: 1827.383602571787 acc test: 0.17777777777777778\n",
      "loss train: 3165.8123335949317 acc train: 0.24423229912490055\n",
      "loss test: 1224.8284843959382 acc test: 0.2574074074074074\n",
      "loss train: 2451.8715288426847 acc train: 0.3556085918854415\n",
      "loss test: 1037.4148206715586 acc test: 0.3685185185185185\n",
      "loss train: 2147.566235977336 acc train: 0.45664280031821797\n",
      "loss test: 950.2115772355078 acc test: 0.42592592592592593\n",
      "loss train: 1915.7232242224447 acc train: 0.5330151153540175\n",
      "loss test: 873.393844313113 acc test: 0.4777777777777778\n",
      "loss train: 1726.0025209014236 acc train: 0.5974542561654733\n",
      "loss test: 810.2154080586284 acc test: 0.5296296296296297\n",
      "loss train: 1565.379668335176 acc train: 0.6324582338902148\n",
      "loss test: 757.1145958339705 acc test: 0.5555555555555556\n",
      "loss train: 1429.1887564696285 acc train: 0.6610978520286396\n",
      "loss test: 708.664839674708 acc test: 0.5888888888888889\n",
      "loss train: 1315.769064041857 acc train: 0.6976929196499603\n",
      "loss test: 666.3615266021112 acc test: 0.6222222222222222\n",
      "loss train: 1217.8868393337384 acc train: 0.7239459029435164\n",
      "loss test: 630.0245873545978 acc test: 0.6462962962962963\n",
      "loss train: 1129.4723885166145 acc train: 0.7541766109785203\n",
      "loss test: 599.9865346546417 acc test: 0.6592592592592592\n",
      "loss train: 1049.390400684003 acc train: 0.7788385043754972\n",
      "loss test: 573.0349403258035 acc test: 0.674074074074074\n",
      "loss train: 980.8619782033951 acc train: 0.7971360381861575\n",
      "loss test: 550.2313648925117 acc test: 0.6833333333333333\n",
      "loss train: 921.9646851014744 acc train: 0.8066825775656324\n",
      "loss test: 529.9692596447621 acc test: 0.6925925925925925\n",
      "loss train: 870.4257293781959 acc train: 0.8194112967382657\n",
      "loss test: 512.5235289555591 acc test: 0.6962962962962963\n",
      "loss train: 824.4765467619526 acc train: 0.8337311058074781\n",
      "loss test: 495.0253090157779 acc test: 0.7092592592592593\n",
      "loss train: 782.3183397124856 acc train: 0.8456642800318218\n",
      "loss test: 478.58630892067083 acc test: 0.7203703703703703\n",
      "loss train: 743.9292157498816 acc train: 0.8536197295147175\n",
      "loss test: 463.382080511351 acc test: 0.7222222222222222\n",
      "loss train: 708.508127342206 acc train: 0.8607796340493238\n",
      "loss test: 448.2432648265623 acc test: 0.7351851851851852\n",
      "loss train: 675.3726074819541 acc train: 0.86793953858393\n",
      "loss test: 434.77745185528596 acc test: 0.7407407407407407\n",
      "loss train: 645.3461244012325 acc train: 0.8758949880668258\n",
      "loss test: 423.0329191917569 acc test: 0.7462962962962963\n",
      "loss train: 617.8378099580035 acc train: 0.8790771678599841\n",
      "loss test: 412.10875716078215 acc test: 0.7555555555555555\n",
      "loss train: 593.0357498491135 acc train: 0.8846459824980112\n",
      "loss test: 402.31410980720284 acc test: 0.7537037037037037\n",
      "loss train: 570.2857664189994 acc train: 0.8894192521877486\n",
      "loss test: 393.2991679708672 acc test: 0.7648148148148148\n",
      "loss train: 549.0315573768307 acc train: 0.8933969769291965\n",
      "loss test: 384.9111372556432 acc test: 0.7722222222222223\n",
      "loss train: 529.3417607387506 acc train: 0.8973747016706444\n",
      "loss test: 377.0640561503442 acc test: 0.774074074074074\n",
      "loss train: 510.92081753328625 acc train: 0.8997613365155132\n",
      "loss test: 369.5271435928908 acc test: 0.7777777777777778\n",
      "loss train: 493.4243288083418 acc train: 0.9061256961018298\n",
      "loss test: 362.22579128776175 acc test: 0.7870370370370371\n",
      "loss train: 476.54585449139245 acc train: 0.9093078758949881\n",
      "loss test: 355.1006192777376 acc test: 0.7925925925925926\n",
      "loss train: 460.57564443883894 acc train: 0.9116945107398569\n",
      "loss test: 348.11577426935446 acc test: 0.7944444444444444\n",
      "loss train: 445.6792698738803 acc train: 0.9164677804295943\n",
      "loss test: 341.398011835931 acc test: 0.7944444444444444\n",
      "loss train: 431.7371890160992 acc train: 0.918854415274463\n",
      "loss test: 335.00736714878735 acc test: 0.8037037037037037\n",
      "loss train: 418.5565438296919 acc train: 0.9204455051710422\n",
      "loss test: 328.9016663056811 acc test: 0.8092592592592592\n",
      "loss train: 405.84289752709117 acc train: 0.92442322991249\n",
      "loss test: 322.9011390205086 acc test: 0.8111111111111111\n",
      "loss train: 393.60969641395434 acc train: 0.9276054097056484\n",
      "loss test: 316.92817381694306 acc test: 0.8185185185185185\n",
      "loss train: 381.783517268521 acc train: 0.9323786793953859\n",
      "loss test: 311.2511521265016 acc test: 0.8222222222222222\n",
      "loss train: 370.62122937370646 acc train: 0.9347653142402546\n",
      "loss test: 305.9495030682965 acc test: 0.8240740740740741\n",
      "loss train: 360.09839378820885 acc train: 0.9371519490851233\n",
      "loss test: 300.9333005765477 acc test: 0.8296296296296296\n",
      "loss train: 350.0742216083924 acc train: 0.939538583929992\n",
      "loss test: 296.15281632827094 acc test: 0.8314814814814815\n",
      "loss train: 340.39164557363466 acc train: 0.9411296738265712\n",
      "loss test: 291.61125609898636 acc test: 0.8351851851851851\n",
      "loss train: 330.9547954665508 acc train: 0.9451073985680191\n",
      "loss test: 287.30408307899523 acc test: 0.8407407407407408\n",
      "loss train: 321.9867991351636 acc train: 0.9474940334128878\n",
      "loss test: 283.1744057404578 acc test: 0.8425925925925926\n",
      "loss train: 313.67943337655174 acc train: 0.9482895783611774\n",
      "loss test: 279.2554344414129 acc test: 0.8518518518518519\n",
      "loss train: 305.9865023944868 acc train: 0.9498806682577565\n",
      "loss test: 275.4791485442899 acc test: 0.8555555555555555\n",
      "loss train: 298.69475632045857 acc train: 0.9506762132060461\n",
      "loss test: 271.84335894848726 acc test: 0.8574074074074074\n",
      "loss train: 291.7419835197084 acc train: 0.9522673031026253\n",
      "loss test: 268.3811793266249 acc test: 0.8592592592592593\n",
      "loss train: 285.14530644386195 acc train: 0.9562450278440732\n",
      "loss test: 265.02523636424826 acc test: 0.8592592592592593\n",
      "loss train: 278.86511156118104 acc train: 0.9594272076372315\n",
      "loss test: 261.77275970738407 acc test: 0.8574074074074074\n",
      "loss train: 272.8376638845409 acc train: 0.960222752585521\n",
      "loss test: 258.7143389801556 acc test: 0.8592592592592593\n",
      "loss train: 267.0088928120865 acc train: 0.9610182975338106\n",
      "loss test: 255.86213368490039 acc test: 0.8629629629629629\n",
      "loss train: 261.3062307822282 acc train: 0.9626093874303898\n",
      "loss test: 253.1883435779241 acc test: 0.8648148148148148\n",
      "loss train: 255.70019315678607 acc train: 0.9626093874303898\n",
      "loss test: 250.6397393437848 acc test: 0.8666666666666667\n",
      "loss train: 250.234082059336 acc train: 0.9634049323786794\n",
      "loss test: 248.26617425709654 acc test: 0.8685185185185185\n",
      "loss train: 245.06576635730818 acc train: 0.964200477326969\n",
      "loss test: 246.1493895397552 acc test: 0.8685185185185185\n",
      "loss train: 240.17220431653772 acc train: 0.9673826571201273\n",
      "loss test: 244.18038109331576 acc test: 0.8703703703703703\n",
      "loss train: 235.4923883391043 acc train: 0.9681782020684169\n",
      "loss test: 242.26850529061585 acc test: 0.8740740740740741\n",
      "loss train: 230.98966477997044 acc train: 0.9689737470167065\n",
      "loss test: 240.35500550627026 acc test: 0.8722222222222222\n",
      "loss train: 226.61628511397086 acc train: 0.9697692919649961\n",
      "loss test: 238.38884618754702 acc test: 0.8740740740740741\n",
      "loss train: 222.32928635976614 acc train: 0.9697692919649961\n",
      "loss test: 236.36877475353756 acc test: 0.8777777777777778\n",
      "loss train: 218.1226565485261 acc train: 0.9705648369132857\n",
      "loss test: 234.32457351229195 acc test: 0.8814814814814815\n",
      "loss train: 213.98275581529245 acc train: 0.9705648369132857\n",
      "loss test: 232.43662138264406 acc test: 0.8814814814814815\n",
      "loss train: 209.98545811394234 acc train: 0.9697692919649961\n",
      "loss test: 230.83982622319394 acc test: 0.8814814814814815\n",
      "loss train: 206.1406373620406 acc train: 0.9697692919649961\n",
      "loss test: 229.45595084689592 acc test: 0.8814814814814815\n",
      "loss train: 202.29893068489483 acc train: 0.9705648369132857\n",
      "loss test: 227.8692341786855 acc test: 0.8814814814814815\n",
      "loss train: 198.40196349431716 acc train: 0.9713603818615751\n",
      "loss test: 226.29837070932518 acc test: 0.8814814814814815\n",
      "loss train: 194.52593185864183 acc train: 0.9721559268098647\n",
      "loss test: 224.7820450388939 acc test: 0.8814814814814815\n",
      "loss train: 190.80743545950529 acc train: 0.9729514717581543\n",
      "loss test: 223.23506984597952 acc test: 0.8814814814814815\n",
      "loss train: 187.12239374748907 acc train: 0.9729514717581543\n",
      "loss test: 221.76226726565352 acc test: 0.8814814814814815\n",
      "loss train: 183.72183245389067 acc train: 0.9753381066030231\n",
      "loss test: 220.36649260062512 acc test: 0.8814814814814815\n",
      "loss train: 180.53358385518325 acc train: 0.9753381066030231\n",
      "loss test: 219.03599535394275 acc test: 0.8814814814814815\n",
      "loss train: 177.5075087256343 acc train: 0.9761336515513126\n",
      "loss test: 217.7695278761002 acc test: 0.8833333333333333\n",
      "loss train: 174.62203365478928 acc train: 0.9777247414478918\n",
      "loss test: 216.58471519144885 acc test: 0.8851851851851852\n",
      "loss train: 171.82456392703784 acc train: 0.9777247414478918\n",
      "loss test: 215.4807821847681 acc test: 0.8851851851851852\n",
      "loss train: 169.07914793239064 acc train: 0.9777247414478918\n",
      "loss test: 214.4431874099306 acc test: 0.8851851851851852\n",
      "loss train: 166.36583107790673 acc train: 0.9785202863961814\n",
      "loss test: 213.45274080112227 acc test: 0.8851851851851852\n",
      "loss train: 163.68737656909846 acc train: 0.979315831344471\n",
      "loss test: 212.49526153787872 acc test: 0.8870370370370371\n",
      "loss train: 161.05108225215565 acc train: 0.979315831344471\n",
      "loss test: 211.56721747826998 acc test: 0.8870370370370371\n",
      "loss train: 158.44222410998435 acc train: 0.979315831344471\n",
      "loss test: 210.6691102214477 acc test: 0.8870370370370371\n",
      "loss train: 155.87398396617309 acc train: 0.9809069212410502\n",
      "loss test: 209.78439957507996 acc test: 0.8888888888888888\n",
      "loss train: 153.40065214995417 acc train: 0.9809069212410502\n",
      "loss test: 208.8842064593393 acc test: 0.8888888888888888\n",
      "train completed!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    # train\n",
    "    \n",
    "    Y_pred = []\n",
    "    for x, y in zip(X_train, Y_train):\n",
    "\n",
    "        # forward\n",
    "        x = x.reshape(-1, 1)\n",
    "\n",
    "        # layer 1\n",
    "        net1 = x.T @ W1 + B1\n",
    "        out1 = sigmoid(net1)\n",
    "\n",
    "        # layer 2\n",
    "        net2 = out1 @ W2 + B2\n",
    "        out2 = sigmoid(net2)\n",
    "\n",
    "        # layer 3\n",
    "        net3 = out2 @ W3 + B3\n",
    "        out3 = softmax(net3)\n",
    "\n",
    "        y_pred = out3\n",
    "        Y_pred.append(y_pred.T)\n",
    "\n",
    "        # back propagation\n",
    "\n",
    "        # layer 3\n",
    "        error = -2 * (y - y_pred)\n",
    "        grad_W3 = out2.T @ error\n",
    "        grad_B3 = error\n",
    "\n",
    "        # layer 2\n",
    "        error = error @ W3.T * out2 * (1 - out2)\n",
    "        grad_W2 = out1.T @ error\n",
    "        grad_B2 = error\n",
    "\n",
    "        # layer 1\n",
    "        error = error @ W2.T * out1 * (1 - out1)\n",
    "        grad_W1 = x @ error\n",
    "        grad_B1 = error\n",
    "\n",
    "        # update\n",
    "\n",
    "        # layer 1\n",
    "        W1 = W1 - η * grad_W1\n",
    "        B1 = B1 - η * grad_B1\n",
    "        \n",
    "        # layer 2\n",
    "        W2 = W2 - η * grad_W2\n",
    "        B2 = B2 - η * grad_B2\n",
    "\n",
    "        # layer 3\n",
    "        W3 = W3 - η * grad_W3\n",
    "        B3 = B3 - η * grad_B3\n",
    "\n",
    "    Y_pred = np.array(Y_pred).reshape(-1, 10)\n",
    "    loss_train = cross_entropy_error(Y_pred, Y_train)\n",
    "    acc_train = np.mean(np.argmax(Y_pred, axis=1) == np.argmax(Y_train, axis=1))\n",
    "    \n",
    "    # test\n",
    "\n",
    "    Y_pred = []\n",
    "    for x, y in zip(X_test, Y_test):\n",
    "\n",
    "        # forward\n",
    "        x = x.reshape(-1, 1)\n",
    "\n",
    "        # layer 1\n",
    "        net1 = x.T @ W1 + B1\n",
    "        out1 = sigmoid(net1)\n",
    "\n",
    "        # layer 2\n",
    "        net2 = out1 @ W2 + B2\n",
    "        out2 = sigmoid(net2)\n",
    "\n",
    "        # layer 3\n",
    "        net3 = out2 @ W3 + B3\n",
    "        out3 = softmax(net3)\n",
    "\n",
    "        y_pred = out3\n",
    "        Y_pred.append(y_pred.T)\n",
    "\n",
    "    Y_pred = np.array(Y_pred).reshape(-1, 10)\n",
    "    loss_test = cross_entropy_error(Y_pred, Y_test)\n",
    "    acc_test = np.mean(np.argmax(Y_pred, axis=1) == np.argmax(Y_test, axis=1))\n",
    "\n",
    "    print('loss train:', loss_train, 'acc train:', acc_train)\n",
    "    print('loss test:', loss_test, 'acc test:', acc_test)\n",
    "\n",
    "print('train completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

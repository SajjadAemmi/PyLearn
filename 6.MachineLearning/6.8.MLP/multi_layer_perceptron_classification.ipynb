{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1257, 64), (1257, 10), (540, 64), (540, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data\n",
    "Y = digits.target\n",
    "Y = np.eye(10)[Y]  # one hot\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "\n",
    "def cross_entropy_error(Y_pred, Y_gt):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(Y_gt * np.log(Y_pred + delta))\n",
    "\n",
    "\n",
    "def root_mean_squired_error(Y_pred, Y_gt):\n",
    "    return np.sqrt(np.mean((Y_pred - Y_gt) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = X_train.shape[1]\n",
    "H1 = 128\n",
    "H2 = 32\n",
    "D_out = Y_train.shape[1]\n",
    "\n",
    "Î· = 0.001\n",
    "epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, W2, W3 = np.random.randn(D_in, H1), np.random.randn(H1, H2), np.random.randn(H2, D_out)\n",
    "B1, B2, B3 = np.random.randn(H1), np.random.randn(H2), np.random.randn(D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss train: 0.05960910950680177 acc train: 0.9880668257756563\n",
      "loss test: 0.13339305748281843 acc test: 0.8722222222222222\n",
      "loss train: 0.05899907634683252 acc train: 0.9880668257756563\n",
      "loss test: 0.13309690690876194 acc test: 0.8722222222222222\n",
      "loss train: 0.05840064290708421 acc train: 0.9888623707239459\n",
      "loss test: 0.13280878829732137 acc test: 0.8722222222222222\n",
      "loss train: 0.057815455454107254 acc train: 0.9896579156722355\n",
      "loss test: 0.13253087165548144 acc test: 0.8722222222222222\n",
      "loss train: 0.05724342504345464 acc train: 0.9896579156722355\n",
      "loss test: 0.13226111594858836 acc test: 0.8722222222222222\n",
      "loss train: 0.05668331445497676 acc train: 0.9896579156722355\n",
      "loss test: 0.13199738393927316 acc test: 0.8740740740740741\n",
      "loss train: 0.05613427913029856 acc train: 0.9896579156722355\n",
      "loss test: 0.13173955727345246 acc test: 0.8759259259259259\n",
      "loss train: 0.055596309281293346 acc train: 0.9912490055688147\n",
      "loss test: 0.1314889182826964 acc test: 0.8759259259259259\n",
      "loss train: 0.05506918299579022 acc train: 0.9912490055688147\n",
      "loss test: 0.1312472080410638 acc test: 0.8759259259259259\n",
      "loss train: 0.05455150457278357 acc train: 0.9920445505171042\n",
      "loss test: 0.13101597703032827 acc test: 0.8759259259259259\n",
      "loss train: 0.05404188901231237 acc train: 0.9920445505171042\n",
      "loss test: 0.13079554435080867 acc test: 0.8796296296296297\n",
      "loss train: 0.053541719057159466 acc train: 0.9920445505171042\n",
      "loss test: 0.13058443767732977 acc test: 0.8796296296296297\n",
      "loss train: 0.05305390955355133 acc train: 0.9920445505171042\n",
      "loss test: 0.13038122639944785 acc test: 0.8796296296296297\n",
      "loss train: 0.05257852130837231 acc train: 0.9920445505171042\n",
      "loss test: 0.13018641603328746 acc test: 0.8796296296296297\n",
      "loss train: 0.05211336249167907 acc train: 0.9920445505171042\n",
      "loss test: 0.1300036420462427 acc test: 0.8796296296296297\n",
      "loss train: 0.051655738380990135 acc train: 0.9920445505171042\n",
      "loss test: 0.1298420980391968 acc test: 0.8796296296296297\n",
      "loss train: 0.05120238381130039 acc train: 0.9920445505171042\n",
      "loss test: 0.12972177382757105 acc test: 0.8814814814814815\n",
      "loss train: 0.05074848110121854 acc train: 0.9920445505171042\n",
      "loss test: 0.12967644798984218 acc test: 0.8777777777777778\n",
      "loss train: 0.05029098559717521 acc train: 0.9920445505171042\n",
      "loss test: 0.1297189339745075 acc test: 0.8814814814814815\n",
      "loss train: 0.04983720322961095 acc train: 0.9928400954653938\n",
      "loss test: 0.12983088207281893 acc test: 0.8814814814814815\n",
      "loss train: 0.04938696422290634 acc train: 0.9928400954653938\n",
      "loss test: 0.12982167576620093 acc test: 0.8796296296296297\n",
      "loss train: 0.048951122826799795 acc train: 0.9928400954653938\n",
      "loss test: 0.1296285947778793 acc test: 0.8796296296296297\n",
      "loss train: 0.04852891118454763 acc train: 0.9928400954653938\n",
      "loss test: 0.12942055779662617 acc test: 0.8814814814814815\n",
      "loss train: 0.04811439983907338 acc train: 0.9928400954653938\n",
      "loss test: 0.1292212457141955 acc test: 0.8833333333333333\n",
      "loss train: 0.04770604804943328 acc train: 0.9928400954653938\n",
      "loss test: 0.12903179207979973 acc test: 0.8814814814814815\n",
      "loss train: 0.04730533545673841 acc train: 0.9928400954653938\n",
      "loss test: 0.12884973180214937 acc test: 0.8814814814814815\n",
      "loss train: 0.046913830641146556 acc train: 0.9928400954653938\n",
      "loss test: 0.12867242994830724 acc test: 0.8814814814814815\n",
      "loss train: 0.04653144448402755 acc train: 0.9928400954653938\n",
      "loss test: 0.12849799428190212 acc test: 0.8814814814814815\n",
      "loss train: 0.046156868851204735 acc train: 0.9928400954653938\n",
      "loss test: 0.12832453878740857 acc test: 0.8814814814814815\n",
      "loss train: 0.04578802090794984 acc train: 0.9936356404136834\n",
      "loss test: 0.12814943299315826 acc test: 0.8833333333333333\n",
      "loss train: 0.04542226238543492 acc train: 0.9936356404136834\n",
      "loss test: 0.12796916578822692 acc test: 0.8833333333333333\n",
      "loss train: 0.04505659229275335 acc train: 0.9936356404136834\n",
      "loss test: 0.12777945360161755 acc test: 0.8851851851851852\n",
      "loss train: 0.04468747886300812 acc train: 0.9936356404136834\n",
      "loss test: 0.12757438173227 acc test: 0.8870370370370371\n",
      "loss train: 0.044309266883546736 acc train: 0.9936356404136834\n",
      "loss test: 0.12734131429229237 acc test: 0.8870370370370371\n",
      "loss train: 0.04390989024581389 acc train: 0.9936356404136834\n",
      "loss test: 0.1270524846995743 acc test: 0.8870370370370371\n",
      "loss train: 0.043475171405289385 acc train: 0.9936356404136834\n",
      "loss test: 0.12672599431217085 acc test: 0.8870370370370371\n",
      "loss train: 0.04304071432816077 acc train: 0.994431185361973\n",
      "loss test: 0.1265182909593804 acc test: 0.8870370370370371\n",
      "loss train: 0.04264868660932475 acc train: 0.994431185361973\n",
      "loss test: 0.1263975086693701 acc test: 0.8870370370370371\n",
      "loss train: 0.042282647821761526 acc train: 0.9952267303102625\n",
      "loss test: 0.12629728236403173 acc test: 0.8870370370370371\n",
      "loss train: 0.041935675785149 acc train: 0.9952267303102625\n",
      "loss test: 0.1262011990045158 acc test: 0.8870370370370371\n",
      "loss train: 0.041603344908822955 acc train: 0.9960222752585521\n",
      "loss test: 0.1261058172528193 acc test: 0.8870370370370371\n",
      "loss train: 0.04128234886152961 acc train: 0.9960222752585521\n",
      "loss test: 0.12601069524776112 acc test: 0.8870370370370371\n",
      "loss train: 0.040970357459836376 acc train: 0.9960222752585521\n",
      "loss test: 0.12591615957423807 acc test: 0.8870370370370371\n",
      "loss train: 0.04066564291294223 acc train: 0.9960222752585521\n",
      "loss test: 0.125822709605967 acc test: 0.8870370370370371\n",
      "loss train: 0.040366865749334374 acc train: 0.9960222752585521\n",
      "loss test: 0.12573082728741264 acc test: 0.8870370370370371\n",
      "loss train: 0.04007296535292007 acc train: 0.9968178202068417\n",
      "loss test: 0.1256409135565215 acc test: 0.8870370370370371\n",
      "loss train: 0.03978310092991154 acc train: 0.9968178202068417\n",
      "loss test: 0.12555327867712698 acc test: 0.8870370370370371\n",
      "loss train: 0.039496609997566344 acc train: 0.9968178202068417\n",
      "loss test: 0.12546815051799542 acc test: 0.8870370370370371\n",
      "loss train: 0.039212967952176124 acc train: 0.9968178202068417\n",
      "loss test: 0.12538566055825578 acc test: 0.8870370370370371\n",
      "loss train: 0.03893175018119016 acc train: 0.9968178202068417\n",
      "loss test: 0.12530576655358594 acc test: 0.8870370370370371\n",
      "loss train: 0.03865260913643826 acc train: 0.9968178202068417\n",
      "loss test: 0.12522809571538673 acc test: 0.8870370370370371\n",
      "loss train: 0.038375259243763726 acc train: 0.9968178202068417\n",
      "loss test: 0.1251517397979929 acc test: 0.8851851851851852\n",
      "loss train: 0.038099405815355626 acc train: 0.9968178202068417\n",
      "loss test: 0.12507506278769634 acc test: 0.8851851851851852\n",
      "loss train: 0.037824518478129225 acc train: 0.9968178202068417\n",
      "loss test: 0.12499554615761513 acc test: 0.8833333333333333\n",
      "loss train: 0.03754943874292212 acc train: 0.9976133651551312\n",
      "loss test: 0.12490968837789629 acc test: 0.8833333333333333\n",
      "loss train: 0.03727208980837147 acc train: 0.9976133651551312\n",
      "loss test: 0.12481343500470775 acc test: 0.8833333333333333\n",
      "loss train: 0.036990157718319344 acc train: 0.9976133651551312\n",
      "loss test: 0.12470506013027027 acc test: 0.8833333333333333\n",
      "loss train: 0.03670418460770834 acc train: 0.9976133651551312\n",
      "loss test: 0.12459128032395879 acc test: 0.8851851851851852\n",
      "loss train: 0.03642044445199824 acc train: 0.9984089101034208\n",
      "loss test: 0.12448408814558531 acc test: 0.8851851851851852\n",
      "loss train: 0.036144432334377675 acc train: 0.9984089101034208\n",
      "loss test: 0.124384492804663 acc test: 0.8851851851851852\n",
      "loss train: 0.0358740258025234 acc train: 0.9984089101034208\n",
      "loss test: 0.12428463307963623 acc test: 0.8851851851851852\n",
      "loss train: 0.03560352633030307 acc train: 0.9984089101034208\n",
      "loss test: 0.12417789588059214 acc test: 0.8851851851851852\n",
      "loss train: 0.0353264603841234 acc train: 0.9984089101034208\n",
      "loss test: 0.12406306192104072 acc test: 0.8870370370370371\n",
      "loss train: 0.0350382743343169 acc train: 0.9984089101034208\n",
      "loss test: 0.12394834714742754 acc test: 0.8870370370370371\n",
      "loss train: 0.03474402573540615 acc train: 0.9984089101034208\n",
      "loss test: 0.12384180938923976 acc test: 0.8870370370370371\n",
      "loss train: 0.03445308935887949 acc train: 0.9984089101034208\n",
      "loss test: 0.12374045069936515 acc test: 0.8870370370370371\n",
      "loss train: 0.034168221773809584 acc train: 0.9992044550517104\n",
      "loss test: 0.12363844620188934 acc test: 0.8870370370370371\n",
      "loss train: 0.033890766238754276 acc train: 0.9992044550517104\n",
      "loss test: 0.12353372017474107 acc test: 0.8870370370370371\n",
      "loss train: 0.03362205528091304 acc train: 0.9992044550517104\n",
      "loss test: 0.12342754029359843 acc test: 0.8870370370370371\n",
      "loss train: 0.033363643336977056 acc train: 0.9992044550517104\n",
      "loss test: 0.12332247851010057 acc test: 0.8870370370370371\n",
      "loss train: 0.033116307856793745 acc train: 0.9992044550517104\n",
      "loss test: 0.12322059422784494 acc test: 0.8870370370370371\n",
      "loss train: 0.0328794017825653 acc train: 0.9992044550517104\n",
      "loss test: 0.12312268074776488 acc test: 0.8870370370370371\n",
      "loss train: 0.032651398002908366 acc train: 0.9992044550517104\n",
      "loss test: 0.12302854861203487 acc test: 0.8870370370370371\n",
      "loss train: 0.032430660514857344 acc train: 0.9992044550517104\n",
      "loss test: 0.12293759116068934 acc test: 0.8870370370370371\n",
      "loss train: 0.03221580616393667 acc train: 0.9992044550517104\n",
      "loss test: 0.12284915543378025 acc test: 0.8870370370370371\n",
      "loss train: 0.03200575815020588 acc train: 0.9992044550517104\n",
      "loss test: 0.1227626879996994 acc test: 0.8870370370370371\n",
      "loss train: 0.031799694068385634 acc train: 0.9992044550517104\n",
      "loss test: 0.12267775874082003 acc test: 0.8870370370370371\n",
      "loss train: 0.031596981436090424 acc train: 0.9992044550517104\n",
      "loss test: 0.12259403937538278 acc test: 0.8888888888888888\n",
      "loss train: 0.031397125724170444 acc train: 0.9992044550517104\n",
      "loss test: 0.12251127282709501 acc test: 0.8907407407407407\n",
      "loss train: 0.031199732772342535 acc train: 0.9992044550517104\n",
      "loss test: 0.12242924639825667 acc test: 0.8907407407407407\n",
      "train completed!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    # train\n",
    "    \n",
    "    Y_pred = []\n",
    "    for x, y in zip(X_train, Y_train):\n",
    "\n",
    "        # forward\n",
    "        x = x.reshape(-1, 1)\n",
    "\n",
    "        # layer 1\n",
    "        net1 = x.T @ W1 + B1\n",
    "        out1 = sigmoid(net1)\n",
    "\n",
    "        # layer 2\n",
    "        net2 = out1 @ W2 + B2\n",
    "        out2 = sigmoid(net2)\n",
    "\n",
    "        # layer 3\n",
    "        net3 = out2 @ W3 + B3\n",
    "        out3 = softmax(net3)\n",
    "\n",
    "        y_pred = out3\n",
    "        Y_pred.append(y_pred.T)\n",
    "\n",
    "        # back propagation\n",
    "\n",
    "        # layer 3\n",
    "        error = -2 * (y - y_pred)\n",
    "        grad_W3 = out2.T @ error\n",
    "        grad_B3 = error\n",
    "\n",
    "        # layer 2\n",
    "        error = error @ W3.T * out2 * (1 - out2)\n",
    "        grad_W2 = out1.T @ error\n",
    "        grad_B2 = error\n",
    "\n",
    "        # layer 1\n",
    "        error = error @ W2.T * out1 * (1 - out1)\n",
    "        grad_W1 = x @ error\n",
    "        grad_B1 = error\n",
    "\n",
    "        # update\n",
    "\n",
    "        # layer 1\n",
    "        W1 = W1 - Î· * grad_W1\n",
    "        B1 = B1 - Î· * grad_B1\n",
    "        \n",
    "        # layer 2\n",
    "        W2 = W2 - Î· * grad_W2\n",
    "        B2 = B2 - Î· * grad_B2\n",
    "\n",
    "        # layer 3\n",
    "        W3 = W3 - Î· * grad_W3\n",
    "        B3 = B3 - Î· * grad_B3\n",
    "\n",
    "    Y_pred = np.array(Y_pred).reshape(-1, 10)\n",
    "    loss_train = root_mean_squired_error(Y_pred, Y_train)\n",
    "    acc_train = np.mean(np.argmax(Y_pred, axis=1) == np.argmax(Y_train, axis=1))\n",
    "    \n",
    "    # test\n",
    "\n",
    "    Y_pred = []\n",
    "    for x, y in zip(X_test, Y_test):\n",
    "\n",
    "        # forward\n",
    "        x = x.reshape(-1, 1)\n",
    "\n",
    "        # layer 1\n",
    "        net1 = x.T @ W1 + B1\n",
    "        out1 = sigmoid(net1)\n",
    "\n",
    "        # layer 2\n",
    "        net2 = out1 @ W2 + B2\n",
    "        out2 = sigmoid(net2)\n",
    "\n",
    "        # layer 3\n",
    "        net3 = out2 @ W3 + B3\n",
    "        out3 = softmax(net3)\n",
    "\n",
    "        y_pred = out3\n",
    "        Y_pred.append(y_pred.T)\n",
    "\n",
    "    Y_pred = np.array(Y_pred).reshape(-1, 10)\n",
    "    loss_test = root_mean_squired_error(Y_pred, Y_test)\n",
    "    acc_test = np.mean(np.argmax(Y_pred, axis=1) == np.argmax(Y_test, axis=1))\n",
    "\n",
    "    print('loss train:', loss_train, 'acc train:', acc_train)\n",
    "    print('loss test:', loss_test, 'acc test:', acc_test)\n",
    "\n",
    "print('train completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sajjad\\AppData\\Local\\Temp\\ipykernel_9372\\2389740904.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread(\"2.png\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "image = image.astype(np.float32)\n",
    "\n",
    "x = image.reshape(-1, 1)\n",
    "\n",
    "# layer 1\n",
    "net1 = x.T @ W1 + B1\n",
    "out1 = sigmoid(net1)\n",
    "\n",
    "# layer 2\n",
    "net2 = out1 @ W2 + B2\n",
    "out2 = sigmoid(net2)\n",
    "\n",
    "# layer 3\n",
    "net3 = out2 @ W3 + B3\n",
    "out3 = softmax(net3)\n",
    "\n",
    "y_pred = out3\n",
    "print(np.argmax(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  6., 15., 15., 15., 16.,  4.,  0.],\n",
       "       [ 0., 12., 15.,  2.,  0., 14., 12.,  0.],\n",
       "       [ 0., 13., 14.,  0.,  0., 13., 14.,  0.],\n",
       "       [ 0., 13., 14.,  0.,  0., 13., 14.,  0.],\n",
       "       [ 0., 12., 14.,  0.,  0., 14., 13.,  0.],\n",
       "       [ 0., 11., 15.,  0.,  2., 16., 13.,  0.],\n",
       "       [ 0.,  5., 16.,  6., 13., 16.,  9.,  0.],\n",
       "       [ 0.,  1., 14., 16., 16., 15.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
